{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/AAPL_long.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-03-21</td>\n",
       "      <td>21.526428</td>\n",
       "      <td>21.773214</td>\n",
       "      <td>21.478930</td>\n",
       "      <td>21.517857</td>\n",
       "      <td>18.534697</td>\n",
       "      <td>644042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-03-22</td>\n",
       "      <td>21.349285</td>\n",
       "      <td>21.589287</td>\n",
       "      <td>21.268929</td>\n",
       "      <td>21.405001</td>\n",
       "      <td>18.437487</td>\n",
       "      <td>623870800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-03-23</td>\n",
       "      <td>21.446072</td>\n",
       "      <td>21.492857</td>\n",
       "      <td>21.228571</td>\n",
       "      <td>21.287500</td>\n",
       "      <td>18.336275</td>\n",
       "      <td>430488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-03-26</td>\n",
       "      <td>21.421070</td>\n",
       "      <td>21.683929</td>\n",
       "      <td>21.259287</td>\n",
       "      <td>21.677856</td>\n",
       "      <td>18.672520</td>\n",
       "      <td>595742000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-03-27</td>\n",
       "      <td>21.649286</td>\n",
       "      <td>22.010000</td>\n",
       "      <td>21.645000</td>\n",
       "      <td>21.945715</td>\n",
       "      <td>18.903240</td>\n",
       "      <td>607129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>121.410004</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>120.419998</td>\n",
       "      <td>123.989998</td>\n",
       "      <td>123.989998</td>\n",
       "      <td>92403800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>2021-03-16</td>\n",
       "      <td>125.699997</td>\n",
       "      <td>127.220001</td>\n",
       "      <td>124.720001</td>\n",
       "      <td>125.570000</td>\n",
       "      <td>125.570000</td>\n",
       "      <td>114740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>2021-03-17</td>\n",
       "      <td>124.050003</td>\n",
       "      <td>125.860001</td>\n",
       "      <td>122.339996</td>\n",
       "      <td>124.760002</td>\n",
       "      <td>124.760002</td>\n",
       "      <td>111437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>122.879997</td>\n",
       "      <td>123.180000</td>\n",
       "      <td>120.320000</td>\n",
       "      <td>120.529999</td>\n",
       "      <td>120.529999</td>\n",
       "      <td>121229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>119.900002</td>\n",
       "      <td>121.430000</td>\n",
       "      <td>119.680000</td>\n",
       "      <td>119.989998</td>\n",
       "      <td>119.989998</td>\n",
       "      <td>185023200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "0     2012-03-21   21.526428   21.773214   21.478930   21.517857   18.534697   \n",
       "1     2012-03-22   21.349285   21.589287   21.268929   21.405001   18.437487   \n",
       "2     2012-03-23   21.446072   21.492857   21.228571   21.287500   18.336275   \n",
       "3     2012-03-26   21.421070   21.683929   21.259287   21.677856   18.672520   \n",
       "4     2012-03-27   21.649286   22.010000   21.645000   21.945715   18.903240   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2259  2021-03-15  121.410004  124.000000  120.419998  123.989998  123.989998   \n",
       "2260  2021-03-16  125.699997  127.220001  124.720001  125.570000  125.570000   \n",
       "2261  2021-03-17  124.050003  125.860001  122.339996  124.760002  124.760002   \n",
       "2262  2021-03-18  122.879997  123.180000  120.320000  120.529999  120.529999   \n",
       "2263  2021-03-19  119.900002  121.430000  119.680000  119.989998  119.989998   \n",
       "\n",
       "         Volume  \n",
       "0     644042000  \n",
       "1     623870800  \n",
       "2     430488800  \n",
       "3     595742000  \n",
       "4     607129600  \n",
       "...         ...  \n",
       "2259   92403800  \n",
       "2260  114740000  \n",
       "2261  111437500  \n",
       "2262  121229700  \n",
       "2263  185023200  \n",
       "\n",
       "[2264 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liu\\anaconda3\\lib\\site-packages\\ta\\trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "C:\\Users\\Liu\\anaconda3\\lib\\site-packages\\ta\\trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>volume_cmf</th>\n",
       "      <th>...</th>\n",
       "      <th>momentum_wr</th>\n",
       "      <th>momentum_ao</th>\n",
       "      <th>momentum_kama</th>\n",
       "      <th>momentum_roc</th>\n",
       "      <th>momentum_ppo</th>\n",
       "      <th>momentum_ppo_signal</th>\n",
       "      <th>momentum_ppo_hist</th>\n",
       "      <th>others_dr</th>\n",
       "      <th>others_dlr</th>\n",
       "      <th>others_cr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-03-21</td>\n",
       "      <td>21.526428</td>\n",
       "      <td>21.773214</td>\n",
       "      <td>21.478930</td>\n",
       "      <td>21.517857</td>\n",
       "      <td>18.534697</td>\n",
       "      <td>644042000</td>\n",
       "      <td>-4.736581e+08</td>\n",
       "      <td>644042000</td>\n",
       "      <td>-0.735446</td>\n",
       "      <td>...</td>\n",
       "      <td>-86.772302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.517857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-48.081084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-03-22</td>\n",
       "      <td>21.349285</td>\n",
       "      <td>21.589287</td>\n",
       "      <td>21.268929</td>\n",
       "      <td>21.405001</td>\n",
       "      <td>18.437487</td>\n",
       "      <td>623870800</td>\n",
       "      <td>-5.675509e+08</td>\n",
       "      <td>20171200</td>\n",
       "      <td>-0.447626</td>\n",
       "      <td>...</td>\n",
       "      <td>-73.016846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.466683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.250425</td>\n",
       "      <td>-0.050085</td>\n",
       "      <td>-0.200340</td>\n",
       "      <td>-0.524476</td>\n",
       "      <td>-0.525856</td>\n",
       "      <td>-0.524476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-03-23</td>\n",
       "      <td>21.446072</td>\n",
       "      <td>21.492857</td>\n",
       "      <td>21.228571</td>\n",
       "      <td>21.287500</td>\n",
       "      <td>18.336275</td>\n",
       "      <td>430488800</td>\n",
       "      <td>-8.060638e+08</td>\n",
       "      <td>-410317600</td>\n",
       "      <td>-0.474601</td>\n",
       "      <td>...</td>\n",
       "      <td>-89.180252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.387061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.915886</td>\n",
       "      <td>-0.623245</td>\n",
       "      <td>-2.292641</td>\n",
       "      <td>-0.548942</td>\n",
       "      <td>-0.550454</td>\n",
       "      <td>-1.070539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-03-26</td>\n",
       "      <td>21.421070</td>\n",
       "      <td>21.683929</td>\n",
       "      <td>21.259287</td>\n",
       "      <td>21.677856</td>\n",
       "      <td>18.672520</td>\n",
       "      <td>595742000</td>\n",
       "      <td>-2.273618e+08</td>\n",
       "      <td>185424400</td>\n",
       "      <td>-0.099105</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.508350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.519120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.873601</td>\n",
       "      <td>-1.073316</td>\n",
       "      <td>-1.800285</td>\n",
       "      <td>1.833733</td>\n",
       "      <td>1.817123</td>\n",
       "      <td>0.743564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-03-27</td>\n",
       "      <td>21.649286</td>\n",
       "      <td>22.010000</td>\n",
       "      <td>21.645000</td>\n",
       "      <td>21.945715</td>\n",
       "      <td>18.903240</td>\n",
       "      <td>607129600</td>\n",
       "      <td>1.659085e+08</td>\n",
       "      <td>792554000</td>\n",
       "      <td>0.057185</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.226595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.707766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.659334</td>\n",
       "      <td>-1.390520</td>\n",
       "      <td>-1.268814</td>\n",
       "      <td>1.235634</td>\n",
       "      <td>1.228063</td>\n",
       "      <td>1.988386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>121.410004</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>120.419998</td>\n",
       "      <td>123.989998</td>\n",
       "      <td>123.989998</td>\n",
       "      <td>92403800</td>\n",
       "      <td>1.909831e+09</td>\n",
       "      <td>-6963953200</td>\n",
       "      <td>-0.075606</td>\n",
       "      <td>...</td>\n",
       "      <td>-37.809770</td>\n",
       "      <td>-8.177766</td>\n",
       "      <td>122.579992</td>\n",
       "      <td>2.479544</td>\n",
       "      <td>0.507923</td>\n",
       "      <td>4.789018</td>\n",
       "      <td>-4.281095</td>\n",
       "      <td>2.445674</td>\n",
       "      <td>2.416246</td>\n",
       "      <td>476.219082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>2021-03-16</td>\n",
       "      <td>125.699997</td>\n",
       "      <td>127.220001</td>\n",
       "      <td>124.720001</td>\n",
       "      <td>125.570000</td>\n",
       "      <td>125.570000</td>\n",
       "      <td>114740000</td>\n",
       "      <td>1.873114e+09</td>\n",
       "      <td>-6849213200</td>\n",
       "      <td>-0.064698</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.179860</td>\n",
       "      <td>-6.572736</td>\n",
       "      <td>122.597043</td>\n",
       "      <td>3.554344</td>\n",
       "      <td>0.443500</td>\n",
       "      <td>3.919915</td>\n",
       "      <td>-3.476414</td>\n",
       "      <td>1.274298</td>\n",
       "      <td>1.266247</td>\n",
       "      <td>483.561830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>2021-03-17</td>\n",
       "      <td>124.050003</td>\n",
       "      <td>125.860001</td>\n",
       "      <td>122.339996</td>\n",
       "      <td>124.760002</td>\n",
       "      <td>124.760002</td>\n",
       "      <td>111437500</td>\n",
       "      <td>1.914903e+09</td>\n",
       "      <td>-6960650700</td>\n",
       "      <td>-0.047035</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.654663</td>\n",
       "      <td>-5.377824</td>\n",
       "      <td>122.637736</td>\n",
       "      <td>-2.371077</td>\n",
       "      <td>0.158372</td>\n",
       "      <td>3.167606</td>\n",
       "      <td>-3.009234</td>\n",
       "      <td>-0.645057</td>\n",
       "      <td>-0.647146</td>\n",
       "      <td>479.797524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>122.879997</td>\n",
       "      <td>123.180000</td>\n",
       "      <td>120.320000</td>\n",
       "      <td>120.529999</td>\n",
       "      <td>120.529999</td>\n",
       "      <td>121229700</td>\n",
       "      <td>1.811477e+09</td>\n",
       "      <td>-7081880400</td>\n",
       "      <td>-0.119166</td>\n",
       "      <td>...</td>\n",
       "      <td>-65.467631</td>\n",
       "      <td>-4.957324</td>\n",
       "      <td>122.626108</td>\n",
       "      <td>-3.668481</td>\n",
       "      <td>0.612994</td>\n",
       "      <td>2.656684</td>\n",
       "      <td>-2.043690</td>\n",
       "      <td>-3.390512</td>\n",
       "      <td>-3.449323</td>\n",
       "      <td>460.139418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>119.900002</td>\n",
       "      <td>121.430000</td>\n",
       "      <td>119.680000</td>\n",
       "      <td>119.989998</td>\n",
       "      <td>119.989998</td>\n",
       "      <td>185023200</td>\n",
       "      <td>1.692004e+09</td>\n",
       "      <td>-7266903600</td>\n",
       "      <td>-0.165473</td>\n",
       "      <td>...</td>\n",
       "      <td>-69.784185</td>\n",
       "      <td>-4.499324</td>\n",
       "      <td>122.599483</td>\n",
       "      <td>-1.695887</td>\n",
       "      <td>5.160554</td>\n",
       "      <td>3.157458</td>\n",
       "      <td>2.003096</td>\n",
       "      <td>-0.448022</td>\n",
       "      <td>-0.449029</td>\n",
       "      <td>457.629870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "0     2012-03-21   21.526428   21.773214   21.478930   21.517857   18.534697   \n",
       "1     2012-03-22   21.349285   21.589287   21.268929   21.405001   18.437487   \n",
       "2     2012-03-23   21.446072   21.492857   21.228571   21.287500   18.336275   \n",
       "3     2012-03-26   21.421070   21.683929   21.259287   21.677856   18.672520   \n",
       "4     2012-03-27   21.649286   22.010000   21.645000   21.945715   18.903240   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2259  2021-03-15  121.410004  124.000000  120.419998  123.989998  123.989998   \n",
       "2260  2021-03-16  125.699997  127.220001  124.720001  125.570000  125.570000   \n",
       "2261  2021-03-17  124.050003  125.860001  122.339996  124.760002  124.760002   \n",
       "2262  2021-03-18  122.879997  123.180000  120.320000  120.529999  120.529999   \n",
       "2263  2021-03-19  119.900002  121.430000  119.680000  119.989998  119.989998   \n",
       "\n",
       "         Volume    volume_adi  volume_obv  volume_cmf  ...  momentum_wr  \\\n",
       "0     644042000 -4.736581e+08   644042000   -0.735446  ...   -86.772302   \n",
       "1     623870800 -5.675509e+08    20171200   -0.447626  ...   -73.016846   \n",
       "2     430488800 -8.060638e+08  -410317600   -0.474601  ...   -89.180252   \n",
       "3     595742000 -2.273618e+08   185424400   -0.099105  ...   -17.508350   \n",
       "4     607129600  1.659085e+08   792554000    0.057185  ...    -8.226595   \n",
       "...         ...           ...         ...         ...  ...          ...   \n",
       "2259   92403800  1.909831e+09 -6963953200   -0.075606  ...   -37.809770   \n",
       "2260  114740000  1.873114e+09 -6849213200   -0.064698  ...   -25.179860   \n",
       "2261  111437500  1.914903e+09 -6960650700   -0.047035  ...   -31.654663   \n",
       "2262  121229700  1.811477e+09 -7081880400   -0.119166  ...   -65.467631   \n",
       "2263  185023200  1.692004e+09 -7266903600   -0.165473  ...   -69.784185   \n",
       "\n",
       "      momentum_ao  momentum_kama  momentum_roc  momentum_ppo  \\\n",
       "0        0.000000      21.517857      0.000000      0.000000   \n",
       "1        0.000000      21.466683      0.000000     -0.250425   \n",
       "2        0.000000      21.387061      0.000000     -2.915886   \n",
       "3        0.000000      21.519120      0.000000     -2.873601   \n",
       "4        0.000000      21.707766      0.000000     -2.659334   \n",
       "...           ...            ...           ...           ...   \n",
       "2259    -8.177766     122.579992      2.479544      0.507923   \n",
       "2260    -6.572736     122.597043      3.554344      0.443500   \n",
       "2261    -5.377824     122.637736     -2.371077      0.158372   \n",
       "2262    -4.957324     122.626108     -3.668481      0.612994   \n",
       "2263    -4.499324     122.599483     -1.695887      5.160554   \n",
       "\n",
       "      momentum_ppo_signal  momentum_ppo_hist  others_dr  others_dlr  \\\n",
       "0                0.000000           0.000000 -48.081084    0.000000   \n",
       "1               -0.050085          -0.200340  -0.524476   -0.525856   \n",
       "2               -0.623245          -2.292641  -0.548942   -0.550454   \n",
       "3               -1.073316          -1.800285   1.833733    1.817123   \n",
       "4               -1.390520          -1.268814   1.235634    1.228063   \n",
       "...                   ...                ...        ...         ...   \n",
       "2259             4.789018          -4.281095   2.445674    2.416246   \n",
       "2260             3.919915          -3.476414   1.274298    1.266247   \n",
       "2261             3.167606          -3.009234  -0.645057   -0.647146   \n",
       "2262             2.656684          -2.043690  -3.390512   -3.449323   \n",
       "2263             3.157458           2.003096  -0.448022   -0.449029   \n",
       "\n",
       "       others_cr  \n",
       "0       0.000000  \n",
       "1      -0.524476  \n",
       "2      -1.070539  \n",
       "3       0.743564  \n",
       "4       1.988386  \n",
       "...          ...  \n",
       "2259  476.219082  \n",
       "2260  483.561830  \n",
       "2261  479.797524  \n",
       "2262  460.139418  \n",
       "2263  457.629870  \n",
       "\n",
       "[2264 rows x 90 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ta\n",
    "\n",
    "df = ta.add_all_ta_features(data, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- encoding: utf-8 -*-\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import tushare as ts\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "DAYS_FOR_TRAIN = 5\n",
    "FEATURE_SIZE = 5\n",
    "\n",
    "\n",
    "class LSTM_Regression(nn.Module):\n",
    "    \"\"\"\n",
    "        使用LSTM进行回归\n",
    "        \n",
    "        参数：\n",
    "        - input_size: feature size\n",
    "        - hidden_size: number of hidden units\n",
    "        - output_size: number of output\n",
    "        - num_layers: layers of LSTM to stack\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, _x):\n",
    "        x, _ = self.lstm(_x)  # _x is input, size (seq_len, batch, input_size)\n",
    "        s, b, h = x.shape  # x is output, size (seq_len, batch, hidden_size)\n",
    "        x = x.view(s*b, h)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(s, b, -1)  # 把形状改回来\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_dataset(data, days_for_train=5) -> (np.array, np.array):\n",
    "    \"\"\"\n",
    "        根据给定的序列data，生成数据集\n",
    "        \n",
    "        数据集分为输入和输出，每一个输入的长度为days_for_train，每一个输出的长度为1。\n",
    "        也就是说用days_for_train天的数据，对应下一天的数据。\n",
    "\n",
    "        若给定序列的长度为d，将输出长度为(d-days_for_train+1)个输入/输出对\n",
    "    \"\"\"\n",
    "    dataset_x, dataset_y= [], []\n",
    "    for i in range(len(data)-days_for_train):\n",
    "        _x = []\n",
    "        _x = data[i:(i+days_for_train)]\n",
    "        dataset_x.append(_x)\n",
    "        dataset_y.append(data[i+days_for_train])\n",
    "    return (np.array(dataset_x), np.array(dataset_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    \"\"\"stock market dataset\"\"\"\n",
    "    \n",
    "    def __init(self, csv_file, days_for_train=5, feature_size=5):\n",
    "        self.days_for_train\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train():\n",
    "    data_close = pd.read_csv('data/AAPL_long.csv').iloc[:, 1:1+FEATURE_SIZE].values  # 取上证指数的收盘价的np.ndarray 而不是pd.Series\n",
    "    data_close = data_close.astype('float32')  # 转换数据类型\n",
    "    plt.plot(data_close)\n",
    "    plt.savefig('data.png', format='png', dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # 将价格标准化到0~1\n",
    "    max_value = np.max(data_close)\n",
    "    min_value = np.min(data_close)\n",
    "    data_close = (data_close - min_value) / (max_value - min_value)\n",
    "\n",
    "    dataset_x, dataset_y = create_dataset(data_close, DAYS_FOR_TRAIN)\n",
    "\n",
    "    # 划分训练集和测试集，70%作为训练集\n",
    "    train_size = int(len(dataset_x) * 0.7)\n",
    "\n",
    "    train_x = dataset_x[:train_size]\n",
    "    train_y = dataset_y[:train_size]\n",
    "    # test_x = dataset_x[train_size:]  # 暂时没有用到\n",
    "    # test_y = dataset_y[train_size:]  # 暂时没有用到\n",
    "\n",
    "    # 将数据改变形状，RNN 读入的数据维度是 (seq_size, batch_size, feature_size)\n",
    "    train_x = train_x.reshape(-1, 1, DAYS_FOR_TRAIN)\n",
    "    train_y = train_y.reshape(-1, 1, 1)\n",
    "    # test_x = test_x.reshape(-1, 1, DAYS_FOR_TRAIN)\n",
    "\n",
    "    # 转为pytorch的tensor对象\n",
    "    train_x = torch.from_numpy(train_x)\n",
    "    train_y = torch.from_numpy(train_y)\n",
    "    # test_x = torch.from_numpy(test_x)\n",
    "\n",
    "    model = LSTM_Regression(DAYS_FOR_TRAIN, 8, output_size=1, num_layers=2)\n",
    "\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "    for i in range(200):                   \n",
    "        out = model(train_x)\n",
    "        if i == 1:\n",
    "            print(out.shape, train_y.shape)\n",
    "        loss = loss_function(out, train_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            print('Epoch: {}, Loss:{:.5f}'.format(i+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_test():\n",
    "    # for test\n",
    "    model = model.eval() # 转换成测试模式\n",
    "    # model.load_state_dict(torch.load('model_params.pkl'))  # 读取参数\n",
    "\n",
    "    # 注意这里用的是全集 模型的输出长度会比原数据少DAYS_FOR_TRAIN 填充使长度相等再作图\n",
    "    dataset_x = dataset_x.reshape(-1, 1, DAYS_FOR_TRAIN)  # (seq_size, batch_size, feature_size)\n",
    "    dataset_x = torch.from_numpy(dataset_x)\n",
    "\n",
    "    pred_test = model(dataset_x) # 全量训练集的模型输出 (seq_size, batch_size, output_size)\n",
    "    pred_test = pred_test.view(-1).data.numpy()\n",
    "    pred_test = np.concatenate((np.zeros(DAYS_FOR_TRAIN), pred_test))  # 填充0 使长度相同\n",
    "    assert len(pred_test) == len(data_close)\n",
    "\n",
    "    plt.plot(pred_test, 'r', label='prediction')\n",
    "    plt.plot(data_close, 'b', label='real')\n",
    "    plt.plot((train_size, train_size), (0, 1), 'g--')  # 分割线 左边是训练数据 右边是测试数据的输出\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('result.png', format='png', dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7905, 1, 1]) torch.Size([7905, 1, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1ec8c3e927cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmy_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmy_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3d33db816238>\u001b[0m in \u001b[0;36mmy_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    my_train()\n",
    "    \n",
    "    my_test()\n",
    "\n",
    "    # torch.save(model.state_dict(), 'model_params.pkl')  # 可以保存模型的参数供未来使用\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# def minmaxscaler(data):\n",
    "#     min = np.amin(data)\n",
    "#     max = np.amax(data)\n",
    "#     return (data - min)/(max-min)\n",
    "\n",
    "class GpData(Dataset):\n",
    "    def __init__(self, file_path,step_len):\n",
    "        self.step_len = step_len\n",
    "        temp_data = pd.read_csv('data/AAPL_long.csv')\n",
    "        data = temp_data.iloc[:, 1:]\n",
    "        all_data = ta.add_all_ta_features(data, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True)\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        self.data = scaler.fit_transform(all_data.values).astype(np.float32)\n",
    "        # self.data = np.zeros((len(all_data), 89), dtype=np.float32)\n",
    "        # temp_y = []\n",
    "        # for i, line in enumerate(all_data):\n",
    "        #     line_data = line.strip().split(\",\")[1:-1]\n",
    "        #     for j in range(len(line_data)):\n",
    "        #         self.data[i][j] = float(line_data[j])\n",
    "        # self.data = minmaxscaler(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        x = self.data[item:item+self.step_len]\n",
    "        # y = self.data[item+self.step_len]\n",
    "        y = np.zeros((1),dtype=np.long)\n",
    "        y[0] =  1 if self.data[item+self.step_len][3] > self.data[item+self.step_len-1][3] else 0\n",
    "        t_x = torch.from_numpy(x)\n",
    "        t_y = torch.from_numpy(y)\n",
    "        # t_y = t_y.reshape(1,6)\n",
    "        # t_y = t_y.reshape(2)\n",
    "        t_y = t_y.type(torch.long)\n",
    "        return t_x,t_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]-self.step_len\n",
    "\n",
    "\n",
    "class PriceGpData(Dataset):\n",
    "    def __init__(self, file_path,step_len):\n",
    "        self.step_len = step_len\n",
    "        temp_data = pd.read_csv('data/AAPL_long.csv')\n",
    "        data = temp_data.iloc[:, 1:]\n",
    "        # y_scaler = MinMaxScaler()\n",
    "        # self.y = y_scaler.fit_transform(temp_data.iloc[:, 3].values.reshape(1, -1)).astype(np.float32)\n",
    "        # self.y = temp_data.iloc[:, 3].values\n",
    "\n",
    "\n",
    "        all_data = ta.add_all_ta_features(data, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True)\n",
    "        self.y = all_data.iloc[:, 3]\n",
    "        self.x = all_data.drop(['Close'], axis=1)\n",
    "        x_scaler = MinMaxScaler()\n",
    "        self.scaler = x_scaler\n",
    "\n",
    "        self.data = x_scaler.fit_transform(self.x.values).astype(np.float32)\n",
    "        # self.data = self.x.values.astype(np.float32)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        x = self.data[item:item+self.step_len]\n",
    "        # y = self.data[item+self.step_len]\n",
    "        y = np.zeros((1),dtype=np.float32)\n",
    "        y[0] = self.y[item+self.step_len]\n",
    "        t_x = torch.from_numpy(x)\n",
    "        t_y = torch.from_numpy(y)\n",
    "        # t_y = t_y.reshape(1,6)\n",
    "        # t_y = t_y.reshape(2)\n",
    "        # t_y = t_y.type(torch.long)\n",
    "        return t_x,t_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]-self.step_len\n",
    "\n",
    "class GpModel(nn.Module):\n",
    "    def __init__(self, input_size,output_size,step_len):\n",
    "        super(GpModel, self).__init__()\n",
    "        self.step_len = step_len\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_size,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out = nn.Linear(self.step_len,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layer(x)\n",
    "        y = y.reshape(y.shape[0],1,self.step_len)\n",
    "        res = self.out(y)\n",
    "        res = res.reshape(res.shape[0],2)\n",
    "        # print(res.shape)\n",
    "        # res = res.type(torch.long)\n",
    "        return res\n",
    "\n",
    "\n",
    "class LstmModel(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size=1, num_classes=2, output_size=2,step_len=50):\n",
    "        super(LstmModel, self).__init__()\n",
    "        self.step_len = step_len\n",
    "        self.num_layers = 4\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(step_len, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x,_ = self.lstm(x)\n",
    "        x = x.transpose(0,1)\n",
    "        x = x.transpose(1,2)\n",
    "        # print(x.shape)\n",
    "        out = self.fc(x) \n",
    "        out = out.reshape(x.shape[0],2)\n",
    "        return out\n",
    "\n",
    "class PriceLstmModel(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size=64, num_classes=1, output_size=2,step_len=50):\n",
    "        super(PriceLstmModel, self).__init__()\n",
    "        self.step_len = step_len\n",
    "        self.num_layers = 4\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(40, 1)\n",
    "        self.fc1 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x,_ = self.lstm(x)\n",
    "        # print(x.shape)\n",
    "        x = x.transpose(0,1)\n",
    "        x = x.transpose(1,2)\n",
    "        # print(x.shape)\n",
    "        out = self.fc(x) \n",
    "        out = out.transpose(1, 2)\n",
    "        out = self.fc1(out)\n",
    "        # print(out.shape)\n",
    "        # out = out\n",
    "        # out = out.reshape(x.shape[0],2)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    " \n",
    "    def __init__(self, input_size):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)  # None 表示 hidden state 会用全0的 state\n",
    "        # print(r_out.shape)\n",
    "        out = self.out(r_out[:, :, 0])\n",
    "        print(out.shape)\n",
    "        return out\n",
    "\n",
    "class LSTM_Regression(nn.Module):\n",
    "    \"\"\"\n",
    "        使用LSTM进行回归\n",
    "        \n",
    "        参数：\n",
    "        - input_size: feature size\n",
    "        - hidden_size: number of hidden units\n",
    "        - output_size: number of output\n",
    "        - num_layers: layers of LSTM to stack\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, _x):\n",
    "        x, _ = self.lstm(_x)  # _x is input, size (seq_len, batch, input_size)\n",
    "        s, b, h = x.shape  # x is output, size (seq_len, batch, hidden_size)\n",
    "        x = x.view(s*b, h)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(s, b, -1)  # 把形状改回来\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "class PriceModel(object):\n",
    "    def train(self, file_path,step_len):\n",
    "        data = PriceGpData(file_path,step_len)\n",
    "        D = DataLoader(dataset=data,batch_size=64,shuffle=True)\n",
    "        # model = GpModel(6,2,step_len)\n",
    "        model = PriceLstmModel(88)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "        for ep in range(30):\n",
    "            for i,(x,y) in enumerate(D):\n",
    "                x = x.transpose(0,1)\n",
    "                pre_y = model(x)\n",
    "                y = y.reshape(y.shape[0])\n",
    "                # print(pre_y.shape, y.shape)\n",
    "                loss = criterion(pre_y,y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if i%100 == 0:\n",
    "                    print('Epoch: {}, Loss: {:.5f}'.format(ep + 1, loss))\n",
    "        \n",
    "        return model, step_len, data\n",
    "            # self.test(model,file_path,step_len, data)\n",
    "\n",
    "\n",
    "    def test(self, model,step_len, test_data):\n",
    "        # model = model.eval() # 转换成测试模式\n",
    "        # # 注意这里用的是全集 模型的输出长度会比原数据少DAYS_FOR_TRAIN 填充使长度相等再作图\n",
    "        # dataset_x = test_data.x.values.reshape(-1, 1, 88).astype('float32')  # (seq_size, batch_size, feature_size)\n",
    "        # dataset_x = torch.from_numpy(dataset_x)\n",
    "        # pred_test = model(dataset_x) # 全量训练集的模型输出 (seq_size, batch_size, output_size)\n",
    "        # pred_test = pred_test.view(-1).data.numpy()\n",
    "        # pred_test = np.concatenate((np.zeros(88), pred_test))  # 填充0 使长度相同\n",
    "        # print(pred_test)\n",
    "        # return pred_test\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        scaler = MinMaxScaler()\n",
    "        criterion = nn.MSELoss()\n",
    "        # test_data = GpData(file_path, step_len)\n",
    "        test_D = DataLoader(dataset=test_data, batch_size=64)\n",
    "        softmax_func = nn.Softmax(dim=1)\n",
    "        pred_res = []\n",
    "        all_loss = []\n",
    "        \n",
    "\n",
    "        for i,(test_x,test_y) in enumerate(test_D):\n",
    "            print(test_x.shape)\n",
    "            test_x = test_x.transpose(0, 1)\n",
    "            pre_test_y = model(test_x)\n",
    "            print(pre_test_y.shape, test_y.shape)\n",
    "            # pred_res.append([item.item() for item in pre_test_y])\n",
    "\n",
    "            # test_loss = softmax_func(pre_test_y)\n",
    "            test_loss = criterion(pre_test_y, test_y)\n",
    "            # print(pre_test_y.shape, test_y[0])\n",
    "            # print(len(test_loss))\n",
    "            # print(test_loss)\n",
    "            # for i in range(len(pre_test_y)):\n",
    "            #     pred_res.append(torch.argmax(pre_test_y[i]).item() & test_y[i].item())\n",
    "            # pred_res.extend([item for item in torch.argmax(pre_test_y).item() & test_y.item()])\n",
    "            temp_d = pre_test_y.reshape(pre_test_y.shape[0])\n",
    "            # print(temp_d.shape)\n",
    "            pred_res.extend([item.item() for item in temp_d])\n",
    "            all_loss.append(test_loss.item())\n",
    "        # ori_pred = test_data.scaler.inverse_transform(pred_res)\n",
    "        # print(ori_pred)\n",
    "        \n",
    "\n",
    "        def inverse_trans(val):\n",
    "            # print(val)\n",
    "            min_data = test_data.scaler.get_params()[\"feature_range\"][0]\n",
    "            max_data = test_data.scaler.get_params()[\"feature_range\"][1]\n",
    "            std_x = (val-min_data)/(max_data-min_data)\n",
    "            real_x = std_x*(test_data.scaler.data_max_[3]-test_data.scaler.data_min_[3])+test_data.scaler.data_min_[3]\n",
    "            return real_x\n",
    "        \n",
    "        # print(pred_res)\n",
    "        real_pred = [inverse_trans(item) for item in pred_res]\n",
    "        print(real_pred[0])\n",
    "        print(len(pred_res), len(test_D))\n",
    "        print('test_prec: {:.5f}'.format(sum(all_loss) / len(test_data)))\n",
    "        return real_pred, pred_res, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liu\\anaconda3\\lib\\site-packages\\ta\\trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "C:\\Users\\Liu\\anaconda3\\lib\\site-packages\\ta\\trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "C:\\Users\\Liu\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:528: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2903.87817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liu\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:528: UserWarning: Using a target size (torch.Size([48])) that is different to the input size (torch.Size([48, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 2254.24536\n",
      "Epoch: 3, Loss: 2267.90625\n",
      "Epoch: 4, Loss: 2837.77563\n",
      "Epoch: 5, Loss: 3332.32788\n",
      "Epoch: 6, Loss: 1726.41870\n",
      "Epoch: 7, Loss: 3602.44287\n",
      "Epoch: 8, Loss: 2585.53271\n",
      "Epoch: 9, Loss: 2233.53369\n",
      "Epoch: 10, Loss: 2127.43823\n",
      "Epoch: 11, Loss: 2386.66992\n",
      "Epoch: 12, Loss: 3028.49878\n",
      "Epoch: 13, Loss: 2621.39307\n",
      "Epoch: 14, Loss: 2615.99976\n",
      "Epoch: 15, Loss: 2564.23022\n",
      "Epoch: 16, Loss: 3083.34595\n",
      "Epoch: 17, Loss: 2524.22974\n",
      "Epoch: 18, Loss: 2394.65991\n",
      "Epoch: 19, Loss: 2790.40381\n",
      "Epoch: 20, Loss: 3320.58789\n",
      "Epoch: 21, Loss: 2667.06177\n",
      "Epoch: 22, Loss: 3080.26782\n",
      "Epoch: 23, Loss: 3385.23486\n",
      "Epoch: 24, Loss: 2465.26929\n",
      "Epoch: 25, Loss: 2119.75977\n",
      "Epoch: 26, Loss: 2254.66504\n",
      "Epoch: 27, Loss: 2656.59229\n",
      "Epoch: 28, Loss: 2953.83960\n",
      "Epoch: 29, Loss: 2961.16919\n",
      "Epoch: 30, Loss: 2916.46362\n"
     ]
    }
   ],
   "source": [
    "cur_model = PriceModel()\n",
    "trained_model, step_len, data = cur_model.train(file_path=\"data\\\\AAPL_long.csv\",step_len=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
